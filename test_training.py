"""
Script para testar o treinamento do modelo - VERSÃƒO CORRIGIDA
Execute este arquivo para treinar e testar o modelo completo
"""
import os
import sys
import warnings
warnings.filterwarnings('ignore')

# Adicionar src ao path
sys.path.append('src')

print("ğŸš€ TESTE DE TREINAMENTO DO MODELO")
print("=" * 60)

# Verificar estrutura de pastas
print("\nğŸ“ Verificando estrutura de pastas...")
pastas_necessarias = ['data', 'src/model', 'models']
for pasta in pastas_necessarias:
    if not os.path.exists(pasta):
        os.makedirs(pasta, exist_ok=True)
        print(f"   âœ… Pasta '{pasta}' criada")
    else:
        print(f"   âœ… Pasta '{pasta}' existe")

# Verificar arquivos de dados
print("\nğŸ“Š Verificando arquivos de dados...")
if os.path.exists('data/projetos.csv'):
    print("   âœ… data/projetos.csv encontrado")
elif os.path.exists('Project Management Dataset.csv'):
    print("   âœ… Project Management Dataset.csv encontrado")
    print("   ğŸ“Œ Usando arquivo na raiz do projeto")
else:
    print("   âŒ Arquivo de dados NÃƒO encontrado!")
    print("   ğŸ“Œ Certifique-se de que existe 'data/projetos.csv' ou 'Project Management Dataset.csv'")
    sys.exit(1)

# Importar e executar treinamento
print("\nğŸ¤– Iniciando treinamento...")
try:
    # âœ… CORREÃ‡ÃƒO: Ajustar para receber 5 valores de retorno
    if os.path.exists('train_corrigido.py'):
        # Usar versÃ£o corrigida se existir
        import importlib.util
        spec = importlib.util.spec_from_file_location("train_corrigido", "train_corrigido.py")
        train_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(train_module)
        modelo, scaler, label_encoders, resultados, threshold = train_module.main()
        print(f"\nâœ… TREINAMENTO CONCLUÃDO COM SUCESSO! (Threshold: {threshold})")
    else:
        # Fallback para versÃ£o original
        from src.model import train
        resultado = train.main()
        if len(resultado) == 5:
            modelo, scaler, label_encoders, resultados, threshold = resultado
        else:
            modelo, scaler, label_encoders, resultados = resultado
            threshold = 0.5
        print("\nâœ… TREINAMENTO CONCLUÃDO COM SUCESSO!")

except Exception as e:
    print(f"\nâŒ Erro durante o treinamento: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# Testar prediÃ§Ã£o
print("\nğŸ”® Testando prediÃ§Ã£o com exemplo...")
try:
    # âœ… CORREÃ‡ÃƒO: Usar preditor corrigido se existir
    if os.path.exists('predict_corrigido.py'):
        import importlib.util
        spec = importlib.util.spec_from_file_location("predict_corrigido", "predict_corrigido.py")
        predict_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(predict_module)
        preditor = predict_module.PreditorProjetos()
    else:
        from src.model import predict
        preditor = predict.PreditorProjetos()

    # Criar um projeto de teste FAVORÃVEL
    projeto_favoravel = {
        'project_cost': 50000,
        'project_benefit': 500000,  # ROI de 900%!
        'start_date': '2024-01-01',
        'end_date': '2024-06-30',
        'project_type': 'INCOME GENERATION',
        'region': 'North',
        'department': 'eCommerce',
        'complexity': 'Low',        # Baixa complexidade
        'phase': 'Phase 4 - Implement',
        'year': 2024,
        'month': 1
    }

    resultado_favoravel = preditor.prever(projeto_favoravel)

    print("\nğŸ“Š RESULTADO DO TESTE (Projeto FavorÃ¡vel):")
    print(f"   PrediÃ§Ã£o: {'SUCESSO' if resultado_favoravel['sucesso'] else 'FRACASSO'}")
    print(f"   Probabilidade: {resultado_favoravel['probabilidade_sucesso']:.1%}")
    print(f"   ROI esperado: {resultado_favoravel['roi_esperado']:.1%}")
    if 'threshold_usado' in resultado_favoravel:
        print(f"   Threshold usado: {resultado_favoravel['threshold_usado']:.2f}")

    # Testar tambÃ©m um projeto DESFAVORÃVEL para comparaÃ§Ã£o
    projeto_desfavoravel = {
        'project_cost': 500000,
        'project_benefit': 400000,  # ROI negativo!
        'start_date': '2024-01-01',
        'end_date': '2025-12-31',   # 2 anos
        'project_type': 'PROCESS IMPROVEMENT',
        'region': 'South',
        'department': 'Admin & BI',
        'complexity': 'High',       # Alta complexidade
        'phase': 'Phase 1 - Explore',
        'year': 2024,
        'month': 1
    }

    resultado_desfavoravel = preditor.prever(projeto_desfavoravel)

    print("\nğŸ“Š RESULTADO DO TESTE (Projeto DesfavorÃ¡vel):")
    print(f"   PrediÃ§Ã£o: {'SUCESSO' if resultado_desfavoravel['sucesso'] else 'FRACASSO'}")
    print(f"   Probabilidade: {resultado_desfavoravel['probabilidade_sucesso']:.1%}")
    print(f"   ROI esperado: {resultado_desfavoravel['roi_esperado']:.1%}")

    # Verificar se o modelo estÃ¡ funcionando corretamente
    if resultado_favoravel['probabilidade_sucesso'] > resultado_desfavoravel['probabilidade_sucesso']:
        print("\nâœ… SISTEMA FUNCIONANDO CORRETAMENTE!")
        print("   ğŸ“ˆ Projeto favorÃ¡vel tem maior probabilidade de sucesso")
    else:
        print("\nâš ï¸  ATENÃ‡ÃƒO: Projeto favorÃ¡vel tem menor probabilidade que o desfavorÃ¡vel")
        print("   ğŸ“Œ Isso pode indicar que o modelo ainda precisa de ajustes")

except Exception as e:
    print(f"\nâŒ Erro durante teste de prediÃ§Ã£o: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

print("\n" + "=" * 60)
print("ğŸ‰ Todos os testes passaram! O modelo estÃ¡ pronto para uso.")
print("\nğŸ“Œ PrÃ³ximos passos:")
print("   1. Execute 'uv run uvicorn src.api.main:app --reload' para iniciar a API")
print("   2. Execute 'uv run streamlit run src/chatbot/app.py' para o chatbot")

print("\nğŸ“Š RESUMO DOS RESULTADOS:")
print(f"   ğŸ¯ Threshold otimizado: {threshold if 'threshold' in locals() else 'N/A'}")
print(f"   ğŸ“ˆ Projeto favorÃ¡vel: {resultado_favoravel['probabilidade_sucesso']:.1%} de sucesso")
print(f"   ğŸ“‰ Projeto desfavorÃ¡vel: {resultado_desfavoravel['probabilidade_sucesso']:.1%} de sucesso")
